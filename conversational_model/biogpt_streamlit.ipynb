{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuned model(Biogpt) deployment on streamlit using Ngrok"
      ],
      "metadata": {
        "id": "tRnGZ5X35-xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries"
      ],
      "metadata": {
        "id": "nJT1N4al6XS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBZ7YGfP8Xjg",
        "outputId": "2091978b-8728-4712-91af-524967283c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.2\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.29.2)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2023.7.22)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 tokenizers-0.13.3 transformers-4.29.2\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=101323239ceeab0dacc9db49d3dab9aee3f5d552c5e7f127c1d236f9669b1e84\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.29.2\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PrWyvmb-CKPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b197c9-b9db-4c32-bc5a-8303dfa8a7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit file"
      ],
      "metadata": {
        "id": "k9wqwL2Q6djF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the cpu code.\n",
        "#device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#generated = generated.to(device)\n",
        "#model.to(device)\n",
        "#model.eval();\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"HEALTHQUERY\")\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, Trainer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from transformers import BioGptTokenizer, BioGptForCausalLM, TrainerCallback\n",
        "from transformers import pipeline\n",
        "#summarizer_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "#summarizer_knnkar = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")\n",
        "summarizer_sshle = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "import os\n",
        "DEBUG           = False\n",
        "\n",
        "INPUT_DIR       = 'articles'\n",
        "\n",
        "USE_APEX        = True\n",
        "APEX_OPT_LEVEL  = 'O1'\n",
        "\n",
        "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",\n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\"}\n",
        "\n",
        "MAXLEN          = 256  #{768, 1024, 1280, 1600}\n",
        "\n",
        "TRAIN_SIZE      = 0.8\n",
        "\n",
        "if USE_APEX:\n",
        "    TRAIN_BATCHSIZE = 16\n",
        "    BATCH_UPDATE    = 128\n",
        "else:\n",
        "    TRAIN_BATCHSIZE = 8\n",
        "    BATCH_UPDATE    = 256\n",
        "\n",
        "EPOCHS          = 3\n",
        "LR              = 5e-4\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020\n",
        "\n",
        "\n",
        "DEVIDE_BY = 20\n",
        "\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
        "model = BioGptForCausalLM.from_pretrained('/content/drive/MyDrive/All models/biogpt')\n",
        "\n",
        "\n",
        "\n",
        "input_text = st.text_input(\"Please Provide your text:\")\n",
        "title = input_text\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + title + SPECIAL_TOKENS['sep_token']\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "model.eval();\n",
        "from heapq import nsmallest\n",
        "\n",
        "# Generate text\n",
        "\n",
        "if len(input_text)>0:\n",
        "  sample_outputs = model.generate(generated,\n",
        "                                do_sample=True,\n",
        "                                max_length=MAXLEN,\n",
        "                                top_k=10,\n",
        "                                top_p=0.7,\n",
        "                                temperature=0.5,\n",
        "                                repetition_penalty=2.0,\n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "\n",
        "  # Initialize an empty list to store the perplexity and text pairs\n",
        "  perplexity_text_pairs = []\n",
        "\n",
        "\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "      text = tokenizer.decode(sample_output,skip_special_tokens=True)\n",
        "      a = len(title)+25\n",
        "      st.write(a)\n",
        "      st.write((\"{}: {}\\n\\n\".format(i+1,  text[a:])))\n",
        "      # all questions print in above cod\n",
        "  bart_Val=text[a:]\n",
        "  #x=summarizer(bart_Val, max_length=200, min_length=30, do_sample=False)\n",
        "  #st.write('-------Bart summarization-----')\n",
        "  #st.write(x[0]['summary_text'])\n",
        "\n",
        "  #summary=summarizer_knnkar(bart_Val, max_length=200, min_length=30, do_sample=False)\n",
        "  #st.write('-------MEETING_SUMMARY-----')\n",
        "  #st.write(summary[0]['summary_text'])\n",
        "\n",
        "  distl=summarizer_sshle(bart_Val, max_length=200, min_length=30, do_sample=False)\n",
        "  st.write('-------distilbart_cnn_12-6 model -----')\n",
        "  st.write(distl[0]['summary_text'])\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "  st.write('Welcome to GPT2')\n",
        "# Create a \"Regenerate\" button\n",
        "\n",
        "\n",
        "# Display output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpPH-iaw9Bp5",
        "outputId": "cb8f35a7-ddce-4de5-ccfa-e8d610228cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrok Configuration"
      ],
      "metadata": {
        "id": "FwqQf8dU6jSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRgQ6OXOHmzI",
        "outputId": "06274049-e58f-4ff3-af65-b9c29907e886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==5.2.1\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==5.2.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19771 sha256=511d3c3ad86e79dfcf70c7e0ea5fbec94c3baa6dfea054f51a277987c9a71eb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e1/46/8d60711cb43fb2e055fb69bb9964f91c9a5046f7924d2996ac\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pyngrok==5.2.1\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8-mehL_IaeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177583c3-d022-4187-eb84-54c217cf6906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "ngrok.set_auth_token(\"2LoeoedCqusM7VxjDHJfm1b9TJg_5wRMcFESTSHY6u26tUoMj\") #ngrok.com\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5brzkfEIdGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af1b801-8d10-4c19-bda5-ea4076e18fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok: no process found\n"
          ]
        }
      ],
      "source": [
        "!killall ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckp26SgGIfim",
        "outputId": "a430a77f-1e94-4cd9-a4b1-3211951a08b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://243a-34-145-215-26.ngrok-free.app\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ]
    }
  ]
}