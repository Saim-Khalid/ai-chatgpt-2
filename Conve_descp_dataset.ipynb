{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"tSWYe3LMSP-b"},"source":["## GPT2 Training on Coversational and descriptive medical Dataset\n","\n","##### Scope:\n","The Aim of this project is to train the GPT-2 Small model using conversational and descriptive datasets with a focus on medical knowledge. The trained model will be utilized as a Meidical assitant capable of responding to customer queries related to medical topics.\n","\n","##### Objective:\n","- Train the GPT-2 Small model using conversational and descriptive datasets.\n","- Fine-tune the model to specialize in medical knowledge and terminology.\n","- Develop a chatbot application that utilizes the trained model.\n","- Enable the chatbot to understand and respond to various medical queries from customers.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FX5JEIimF82-"},"source":["#### Check GPU memory\n","\n","Due to large dataset and model size distributed traning required.\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:51:50.876088Z","iopub.status.busy":"2023-05-30T06:51:50.875599Z","iopub.status.idle":"2023-05-30T06:51:51.952320Z","shell.execute_reply":"2023-05-30T06:51:51.951049Z","shell.execute_reply.started":"2023-05-30T06:51:50.876045Z"},"id":"nvdLF7jjREuv","outputId":"8de4492f-f896-4c16-9071-b69aba30d023","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue May 30 06:51:51 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n","| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Import Libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:51:58.048269Z","iopub.status.busy":"2023-05-30T06:51:58.047866Z","iopub.status.idle":"2023-05-30T06:52:14.829691Z","shell.execute_reply":"2023-05-30T06:52:14.828402Z","shell.execute_reply.started":"2023-05-30T06:51:58.048217Z"},"id":"IL6-XP7zzH7h","outputId":"eee95bb2-c8b5-477a-dede-c16fec738718","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch version: 1.11.0\n"]}],"source":["import os\n","import io\n","import requests\n","import numpy as np\n","import pandas as pd\n","import re\n","import zipfile\n","import random\n","import time\n","import csv\n","import datetime\n","from itertools import compress\n","from collections import Counter, defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","%%time\n","%%capture\n","!pip install transformers\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n","                         AdamW, get_linear_schedule_with_warmup, \\\n","                         TrainingArguments, BeamScorer, Trainer\n","\n","import torch\n","from torch.utils.data import Dataset, random_split, DataLoader, \\\n","                             RandomSampler, SequentialSampler\n","\n","from IPython.display import clear_output\n","\n","print(f\"PyTorch version: {torch.__version__}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lZDy9RClRiQ3"},"source":["### Configurations\n","\n","Done some configurations which are use in trainig Model \n","- Usage of special tokens at start and end of text\n","- Define maximum lenght of sentence\n","- Define ephocs and other hyperPerameters. "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:55:56.564765Z","iopub.status.busy":"2023-05-30T06:55:56.563593Z","iopub.status.idle":"2023-05-30T06:55:56.574393Z","shell.execute_reply":"2023-05-30T06:55:56.573030Z","shell.execute_reply.started":"2023-05-30T06:55:56.564712Z"},"id":"QILzrXuoRhaF","trusted":true},"outputs":[],"source":["DEBUG           = False\n","\n","\n","USE_APEX        = True\n","APEX_OPT_LEVEL  = 'O1'\n","\n","MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n","\n","UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n","\n","SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n","                    \"eos_token\": \"<|EOS|>\",\n","                    \"unk_token\": \"<|UNK|>\",                    \n","                    \"pad_token\": \"<|PAD|>\",\n","                    \"sep_token\": \"<|SEP|>\"}\n","                    \n","MAXLEN          = 256  #{768, 1024, 1280, 1600}\n","\n","TRAIN_SIZE      = 0.8\n","\n","if USE_APEX:\n","    TRAIN_BATCHSIZE = 16\n","    BATCH_UPDATE    = 5\n","else:\n","    TRAIN_BATCHSIZE = 8\n","    BATCH_UPDATE    = 8\n","\n","EPOCHS          = 5\n","LR              = 5e-4\n","EPS             = 1e-8\n","WARMUP_STEPS    = 1e2\n","\n","SEED            = 2020\n","\n","\n","os.environ['WANDB_DISABLED'] = 'true'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Set Seed Values"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:56:03.303521Z","iopub.status.busy":"2023-05-30T06:56:03.303132Z","iopub.status.idle":"2023-05-30T06:56:03.314170Z","shell.execute_reply":"2023-05-30T06:56:03.313072Z","shell.execute_reply.started":"2023-05-30T06:56:03.303488Z"},"id":"740ZIyZXRbWe","trusted":true},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5RpxktDOHpMI"},"source":["### Load Dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:57:35.268404Z","iopub.status.busy":"2023-05-30T06:57:35.267648Z","iopub.status.idle":"2023-05-30T06:57:37.129007Z","shell.execute_reply":"2023-05-30T06:57:37.127614Z","shell.execute_reply.started":"2023-05-30T06:57:35.268365Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv('/kaggle/input/conversatioanl-dataset/train.csv')\n","test_df = pd.read_csv('/kaggle/input/conversatioanl-dataset/test.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Data Preprocessing "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:57:37.132975Z","iopub.status.busy":"2023-05-30T06:57:37.131426Z","iopub.status.idle":"2023-05-30T06:57:37.226721Z","shell.execute_reply":"2023-05-30T06:57:37.225693Z","shell.execute_reply.started":"2023-05-30T06:57:37.132928Z"},"trusted":true},"outputs":[],"source":["train_df = train_df.dropna()\n","train_df = train_df.astype('str')\n","test_df = test_df.dropna()\n","test_df = test_df.astype('str')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:57:37.403886Z","iopub.status.busy":"2023-05-30T06:57:37.403030Z","iopub.status.idle":"2023-05-30T06:57:37.415584Z","shell.execute_reply":"2023-05-30T06:57:37.414532Z","shell.execute_reply.started":"2023-05-30T06:57:37.403851Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>preprocess_Patient</th>\n","      <th>preprocess_Doctor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Today, my hip joint began to hurt when I walke...</td>\n","      <td>Hello and Welcome to â€˜Ask A Doctor service. I...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>For about two weeks I have had lower back pain...</td>\n","      <td>Thank you for asking Healthcare majic. My nam...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Previously i had a surgery of hie tie of varic...</td>\n","      <td>Hello and .As an Urologist, let me advise you...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>my child, age 10 yrs and weight 26.5 kg, has t...</td>\n","      <td>Anticonvulsant drugs once started are usualll...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I am 62 yrs young, 5ft 4 inches tall &amp; weigh a...</td>\n","      <td>Hello, Well, while adhesions or hiatal hernia...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  preprocess_Patient  \\\n","0  Today, my hip joint began to hurt when I walke...   \n","1  For about two weeks I have had lower back pain...   \n","2  Previously i had a surgery of hie tie of varic...   \n","3  my child, age 10 yrs and weight 26.5 kg, has t...   \n","4  I am 62 yrs young, 5ft 4 inches tall & weigh a...   \n","\n","                                   preprocess_Doctor  \n","0   Hello and Welcome to â€˜Ask A Doctor service. I...  \n","1   Thank you for asking Healthcare majic. My nam...  \n","2   Hello and .As an Urologist, let me advise you...  \n","3   Anticonvulsant drugs once started are usualll...  \n","4   Hello, Well, while adhesions or hiatal hernia...  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Check average length"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:57:54.476304Z","iopub.status.busy":"2023-05-30T06:57:54.475892Z","iopub.status.idle":"2023-05-30T06:57:54.497082Z","shell.execute_reply":"2023-05-30T06:57:54.495853Z","shell.execute_reply.started":"2023-05-30T06:57:54.476271Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["93.833\n"]}],"source":["sum = 0\n","sample_num = 1000\n","for review in train_df.sample(sample_num).iloc[:, 1]:\n","    sum += len(review.split(' '))\n","print(sum/sample_num)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2j0V83HbH6QF"},"source":["### Datasets and loaders"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:58:51.396386Z","iopub.status.busy":"2023-05-30T06:58:51.395605Z","iopub.status.idle":"2023-05-30T06:58:51.409894Z","shell.execute_reply":"2023-05-30T06:58:51.408731Z","shell.execute_reply.started":"2023-05-30T06:58:51.396349Z"},"id":"I8gp0I8JnMEE","trusted":true},"outputs":[],"source":["class myDataset(Dataset):\n","\n","    def __init__(self, data, tokenizer, randomize=True):\n","        self.randomize = randomize\n","        self.tokenizer = tokenizer \n","        self.title     = data.iloc[:, 0].tolist() # it will load the first column of dataframe\n","        self.text      = data.iloc[:, 1].tolist() # it will load the second column  of dataframe\n","\n","\n","    #---------------------------------------------#\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    #---------------------------------------------#\n","    \n","    def __getitem__(self, i):\n","        input = SPECIAL_TOKENS['bos_token'] + self.title[i] + SPECIAL_TOKENS['sep_token'] + self.text[i] + SPECIAL_TOKENS['eos_token'] # Merge col 1 and col 2 with special tokens\n","\n","        encodings_dict = tokenizer(input,                                   \n","                                   truncation=True, \n","                                   max_length=MAXLEN, \n","                                   padding=\"max_length\")   \n","        \n","        input_ids = encodings_dict['input_ids']\n","        attention_mask = encodings_dict['attention_mask']\n","        \n","        return {'label': torch.tensor(input_ids),\n","                'input_ids': torch.tensor(input_ids), \n","                'attention_mask': torch.tensor(attention_mask)}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Split Into Train and Validation"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:58:59.455733Z","iopub.status.busy":"2023-05-30T06:58:59.455344Z","iopub.status.idle":"2023-05-30T06:58:59.462970Z","shell.execute_reply":"2023-05-30T06:58:59.461786Z","shell.execute_reply.started":"2023-05-30T06:58:59.455694Z"},"trusted":true},"outputs":[],"source":["def split_data(data, S=TRAIN_SIZE):\n","    train_data = data.sample(frac = TRAIN_SIZE)\n","    val_data = data.drop(train_data.index)\n","\n","    return train_data, val_data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v3LfEbc5j9Yo"},"source":["### Loading Tokenizer, Config and Model"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:59:10.479590Z","iopub.status.busy":"2023-05-30T06:59:10.479173Z","iopub.status.idle":"2023-05-30T06:59:10.491097Z","shell.execute_reply":"2023-05-30T06:59:10.489626Z","shell.execute_reply.started":"2023-05-30T06:59:10.479555Z"},"id":"knL24TEIX9fl","trusted":true},"outputs":[],"source":["def get_tokenier(special_tokens=None):\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n","\n","    if special_tokens:\n","        tokenizer.add_special_tokens(special_tokens)\n","        print(\"Special tokens added\")\n","    return tokenizer\n","\n","def get_model(tokenizer, special_tokens=None, load_model_path=None):\n","\n","    #GPT2LMHeadModel\n","    if special_tokens:\n","        config = AutoConfig.from_pretrained(MODEL, \n","                                            bos_token_id=tokenizer.bos_token_id,\n","                                            eos_token_id=tokenizer.eos_token_id,\n","                                            sep_token_id=tokenizer.sep_token_id,\n","                                            pad_token_id=tokenizer.pad_token_id,\n","                                            output_hidden_states=False)\n","    else: \n","        config = AutoConfig.from_pretrained(MODEL,                                     \n","                                            pad_token_id=tokenizer.eos_token_id,\n","                                            output_hidden_states=False)    \n","\n","    #----------------------------------------------------------------#\n","    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n","\n","    if special_tokens:\n","        #Special tokens added, model needs to be resized accordingly\n","        model.resize_token_embeddings(len(tokenizer))\n","\n","    if load_model_path:\n","        model.load_state_dict(torch.load(load_model_path))\n","\n","    model.cuda()\n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:59:14.775896Z","iopub.status.busy":"2023-05-30T06:59:14.775472Z","iopub.status.idle":"2023-05-30T06:59:42.618726Z","shell.execute_reply":"2023-05-30T06:59:42.617513Z","shell.execute_reply.started":"2023-05-30T06:59:14.775859Z"},"id":"s2zrELuFTzEG","outputId":"5f881925-1e0e-42a6-fdbc-213faa2365df","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f364406919c40d897f450c390816e65","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3488b933094f9bb075ff9484b62a63","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3623cdc094e4de9bbccfba3005bedda","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78d40354d6714fe687d46c960c8d884b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Special tokens added\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee178720886c48429a980768167a54e9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 16.8 s, sys: 3.25 s, total: 20 s\n","Wall time: 27.8 s\n"]}],"source":["%%time\n","\n","tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n","model = get_model(tokenizer, \n","                  special_tokens=SPECIAL_TOKENS,\n","                #   load_model_path='pytorch_model.bin'\n","                 )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-12T10:05:43.261196Z","iopub.status.busy":"2022-08-12T10:05:43.26027Z","iopub.status.idle":"2022-08-12T10:05:43.268512Z","shell.execute_reply":"2022-08-12T10:05:43.267306Z","shell.execute_reply.started":"2022-08-12T10:05:43.261149Z"},"trusted":true},"outputs":[],"source":["train_data, val_data = split_data(train_df)\n","\n","train_dataset = myDataset(train_data, tokenizer)\n","val_dataset = myDataset(val_data, tokenizer, randomize=False)\n","\n","f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### train and validation dataset"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:59:42.621551Z","iopub.status.busy":"2023-05-30T06:59:42.620810Z","iopub.status.idle":"2023-05-30T06:59:42.639562Z","shell.execute_reply":"2023-05-30T06:59:42.638613Z","shell.execute_reply.started":"2023-05-30T06:59:42.621508Z"},"trusted":true},"outputs":[],"source":["train_dataset = myDataset(train_df, tokenizer)\n","val_dataset = myDataset(test_df, tokenizer, randomize=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LYh9gAM_lAxK"},"source":["### Fine-tune GPT2 using Trainer"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T06:59:54.115181Z","iopub.status.busy":"2023-05-30T06:59:54.114689Z","iopub.status.idle":"2023-05-30T11:50:48.331579Z","shell.execute_reply":"2023-05-30T11:50:48.330398Z","shell.execute_reply.started":"2023-05-30T06:59:54.115136Z"},"id":"GsKQJis8jcCh","outputId":"dcdf82cf-ceaf-4c30-89f0-62d07f99dd74","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 169833\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 160\n","  Gradient Accumulation steps = 5\n","  Total optimization steps = 5305\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5305' max='5305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5305/5305 4:50:41, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>2.193000</td>\n","      <td>2.089931</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2.068300</td>\n","      <td>2.017886</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.990400</td>\n","      <td>1.981005</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.928400</td>\n","      <td>1.954305</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.881700</td>\n","      <td>1.943041</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 18871\n","  Batch size = 32\n","Saving model checkpoint to ./checkpoint-1061\n","Configuration saved in ./checkpoint-1061/config.json\n","Model weights saved in ./checkpoint-1061/pytorch_model.bin\n","tokenizer config file saved in ./checkpoint-1061/tokenizer_config.json\n","Special tokens file saved in ./checkpoint-1061/special_tokens_map.json\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","***** Running Evaluation *****\n","  Num examples = 18871\n","  Batch size = 32\n","Saving model checkpoint to ./checkpoint-2122\n","Configuration saved in ./checkpoint-2122/config.json\n","Model weights saved in ./checkpoint-2122/pytorch_model.bin\n","tokenizer config file saved in ./checkpoint-2122/tokenizer_config.json\n","Special tokens file saved in ./checkpoint-2122/special_tokens_map.json\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","***** Running Evaluation *****\n","  Num examples = 18871\n","  Batch size = 32\n","Saving model checkpoint to ./checkpoint-3183\n","Configuration saved in ./checkpoint-3183/config.json\n","Model weights saved in ./checkpoint-3183/pytorch_model.bin\n","tokenizer config file saved in ./checkpoint-3183/tokenizer_config.json\n","Special tokens file saved in ./checkpoint-3183/special_tokens_map.json\n","Deleting older checkpoint [checkpoint-1061] due to args.save_total_limit\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","***** Running Evaluation *****\n","  Num examples = 18871\n","  Batch size = 32\n","Saving model checkpoint to ./checkpoint-4244\n","Configuration saved in ./checkpoint-4244/config.json\n","Model weights saved in ./checkpoint-4244/pytorch_model.bin\n","tokenizer config file saved in ./checkpoint-4244/tokenizer_config.json\n","Special tokens file saved in ./checkpoint-4244/special_tokens_map.json\n","Deleting older checkpoint [checkpoint-2122] due to args.save_total_limit\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","***** Running Evaluation *****\n","  Num examples = 18871\n","  Batch size = 32\n","Saving model checkpoint to ./checkpoint-5305\n","Configuration saved in ./checkpoint-5305/config.json\n","Model weights saved in ./checkpoint-5305/pytorch_model.bin\n","tokenizer config file saved in ./checkpoint-5305/tokenizer_config.json\n","Special tokens file saved in ./checkpoint-5305/special_tokens_map.json\n","Deleting older checkpoint [checkpoint-3183] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./checkpoint-5305 (score: 1.9430410861968994).\n","Saving model checkpoint to ./\n","Configuration saved in ./config.json\n","Model weights saved in ./pytorch_model.bin\n","tokenizer config file saved in ./tokenizer_config.json\n","Special tokens file saved in ./special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 7h 37min 35s, sys: 29min, total: 8h 6min 35s\n","Wall time: 4h 50min 54s\n"]}],"source":["%%time\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./\",\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=TRAIN_BATCHSIZE,\n","    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n","    gradient_accumulation_steps=BATCH_UPDATE,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy = 'epoch',\n","    fp16=True,\n","    fp16_opt_level=APEX_OPT_LEVEL,\n","    warmup_steps=WARMUP_STEPS,    \n","    learning_rate=LR,\n","    adam_epsilon=EPS,\n","    weight_decay=0.01,        \n","    save_total_limit=1,\n","    load_best_model_at_end=True,\n","    report_to = None,\n",")\n","\n","#---------------------------------------------------#\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,    \n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer\n",")\n","\n","#---------------------------------------------------#\n","trainer.train()\n","trainer.save_model()    "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0azvVPXCx4eM"},"source":["### Generating text with Fine-tuned GPT-2 model\n","\n","After the trainig load trained model and generate some results."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T11:53:42.048888Z","iopub.status.busy":"2023-05-30T11:53:42.048442Z","iopub.status.idle":"2023-05-30T11:53:46.839460Z","shell.execute_reply":"2023-05-30T11:53:46.838336Z","shell.execute_reply.started":"2023-05-30T11:53:42.048843Z"},"id":"dojGngEDRupX","outputId":"beea7b67-8385-40c7-d347-a736fd393949","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.20.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n","loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n","loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.20.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","Assigning <|BOS|> to the bos_token key of the tokenizer\n","Assigning <|EOS|> to the eos_token key of the tokenizer\n","Assigning <|UNK|> to the unk_token key of the tokenizer\n","Assigning <|PAD|> to the pad_token key of the tokenizer\n","Assigning <|SEP|> to the sep_token key of the tokenizer\n","loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50257,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50258,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 50260,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"sep_token_id\": 50261,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.20.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n"]},{"name":"stdout","output_type":"stream","text":["Special tokens added\n"]},{"name":"stderr","output_type":"stream","text":["loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"]}],"source":["tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n","model = get_model(tokenizer, \n","                  special_tokens=SPECIAL_TOKENS,\n","                  load_model_path='/kaggle/working/pytorch_model.bin')"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T12:44:40.652558Z","iopub.status.busy":"2023-05-30T12:44:40.651719Z","iopub.status.idle":"2023-05-30T12:44:40.662359Z","shell.execute_reply":"2023-05-30T12:44:40.661273Z","shell.execute_reply.started":"2023-05-30T12:44:40.652516Z"},"id":"WYCC-ugJJy3A","trusted":true},"outputs":[],"source":["title = \"how to treat asthma ?\"\n","prompt = SPECIAL_TOKENS['bos_token'] + title + SPECIAL_TOKENS['sep_token'] \n","         \n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","device = torch.device(\"cuda\")\n","generated = generated.to(device)\n","\n","model.eval();"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T12:44:42.195321Z","iopub.status.busy":"2023-05-30T12:44:42.194702Z","iopub.status.idle":"2023-05-30T12:44:46.013819Z","shell.execute_reply":"2023-05-30T12:44:46.012636Z","shell.execute_reply.started":"2023-05-30T12:44:42.195284Z"},"id":"g1gM2DvGeh2i","outputId":"e600a352-2cae-477a-8b94-cbc0ef6dcb44","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1: Hello.Thank you for asking at HCM I went through your history and would like make suggestions as follows1 As per my understanding, inhalers are helpful in managing acute exacerbations of wheezing episodes - Salbutamol or Levo-cetirizine can be used 2 However if it is not working better with an oral steroid therapy then a corticosteroid injection may also help 3 If symptoms do worsen despite using salmeterole alone after the treatment sessions than inhaled steroids should never use 4 Also certain types such medications including antihistamines must always take care that they contain only some ingredients which could cause side effects 5 In addition i suggest patients who have already taken prednisone injections regularly over years when there has been no improvement by adding montelukast + levoceterazoline during this time 6 For further information consult pulmonologist online.---> httpswww..askdrsudhilaryalpedsiotherapycenter \n","\n","\n","2: Hi, thanks for query. You should take regular steam inhalation by a doctor who is trained in this type of medicine and you can also go through montelukast one daily at night before going out. Take care \n","\n","\n","3: Hello, Welcome To HealthcareMagic forum. Asthma is a disease caused by allergies and inflammation of airways or mucosa around the lungs called as bronchiolitis which usually occurs due seasonal changes in winters. You can start with steam inhalation twice daily for 2-3 days alongwith regular breathing exercises like - deep inhaling & salbutamol both during morning& evening times while sleeping,you will feel better after taking this medicines but it may take 3 months if not relieved you need some more investigations such As TC  TLC DLC eosinophil count AEC Chest Xray PA view PFT Pulmonary Function Test BMT Spirometry CXR If all these are normal then there seems no connection between asthmatic attackAsthaline therapy + Montelukast+Budesonide combination tablet once per dayFor further information consult an internal medicine physician online --> httpswwwicliniqiagnosticsonlineinternalmedicationphysicianorTAH All your doubts have been clearedThank uDr RatneshwariWish speedy recovery God blessGood luck\n","\n","\n","4: Hi, Welcome To HealthcareMagic forum. Asthma is the allergic condition which causes wheezing and coughing with difficulty in breathing due ti allergy or some other cause like dust etc.. If you have any history of exposure then it will be better if u can take montelukast+ bambuterol tablet daily once for 3-5 days. \n","\n","\n","5: Hello.Thank you for asking at HCM..I went through your history and would like further information regarding my opinion as follows1 What is the type of symptoms, frequency or severity in which a patient develops episodes when they get exposed from outside air-conditioned places where exposure may be more than once per week2 Do any other associated illnesses that occur during these times such allergies4 Are there family members who are affected by this allergyif yes then what specific food items should avoid them5 If not I suggest spirometry with post prandial reversibility testing done regularly6 How long does it take after first episode onset if no prior symptomsthoughtskin prick test will help me identify triggering factors3 Does anyone else has similar complaints also Please revert backIf possible send detailed clinical details so i can guide You betterSince how many days do patients develop attacks whenever on environmentothersInhaler sprays etcYou need proper evaluation before making next choiceHope above answers all questions have been helpfulWish best health ahead \n","\n","\n","6: hi welcome.asthma is a chronic allergic disease, which causes symptoms of wheezing and chest tightness when exposed for long time in the form or environment change like dust, mite etc.you need treatment with bronchodilators such as salbutamol inhalers 2 puffs twice daily before breakfast n dinner alongwith antihistamines levocetrizine + montelukast+citrizinate once at night regularly \n","\n","\n","7: Hi, welcome. I understand your concern about Asthma. As per the guidelines, it is best not use inhalers for a long time as they can cause respiratory problems and also lead ti heart failure or other complications in future.. For prevention you should follow these instructions 1 Use an asthalin based controller with mask whenever possible during night 2 Avoid allergens like dust 3 Take montelukast+levocetrizine tablet once daily at bedtime 4 avoid smoke & pollution \n","\n","\n","8: Hi, welcome. The best treatment for asthmatic attacks is a long acting steroid inhaler and if there are any side effects associated with the use of it then we need not worry about them as they do no harm on their own in future when using steroids but at present you should take care that your body gets rest from all these drugs which will reduce symptoms by improving immunity so better avoid taking those medicines again after consulting an allergist \n","\n","\n","9: Hello.Thank you for asking at HCM I went through your history and would like make suggestions as follows1As per my understanding, it is difficult with this presentation of symptoms because there are many causes that produce such symptomsspecific triggers in patients who have recurrent episodes due respiratory tract infections or chronic obstructive pulmonary disease COPD etc2In general, the treatment options include 1 Antihistamines 2 Steroids 3 Proton Pump Inhibitors 4 ImmunotherapyIf these do not help much better consulting a pulmonologist will be helpfulI hope above information helps youthank-you \n","\n","\n","10: Hi, Welcome To HealthcareMagic Forum. Asthma is a chronic disease which can be treated with homeopathy and ayurvedic medicines like Vati Basti etc., but you need proper medical care from an expert Physician as well who will give correct treatment plan for your ailment. \n","\n","\n"]}],"source":["# Top-p (nucleus) text generation (10 samples):\n","sample_outputs = model.generate(generated, \n","                                do_sample=True,   \n","                                min_length=50, \n","                                max_length=MAXLEN,\n","                                top_k=30,                                 \n","                                top_p=0.7,        \n","                                temperature=0.9,\n","                                repetition_penalty=2.0,\n","                                num_return_sequences=10\n","                                )\n","\n","for i, sample_output in enumerate(sample_outputs):\n","    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    a = len(title)  \n","    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-05-30T12:44:46.016590Z","iopub.status.busy":"2023-05-30T12:44:46.016178Z","iopub.status.idle":"2023-05-30T12:44:49.471200Z","shell.execute_reply":"2023-05-30T12:44:49.469995Z","shell.execute_reply.started":"2023-05-30T12:44:46.016550Z"},"id":"BAmwMuxa3xGW","outputId":"429ec8c0-4c8a-481a-e001-a944c734be49","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1: Hello,Thank you for asking at HCM.I went through your history and would like to make suggestions for you as follows1. Asthma is caused due to broncho-constriction obstruction of smaller airway passages which is indicative of Hyper-responsiveness of air passages.2. I usually suggest my such patients regular montelukast and levocetirizinecetirizinebambuterol inhaler once or twice a day depending upon response.3. Please avoid exposure to dusts, smokes and air pollution as much as possible.4. Were I treating you, I would prescribe you an antihistamine like cetirizinelevocetirizinefexofenadinehydroxyzinepantoprazole before breakfast for 2 weeks.5. Regular steam inhalation with salbutamol can also be helpful.Hope above suggestions will be helpful to you.Should you have any further query, please feel free to ask at HCM.Wish you the best of the health ahead.Thank you & \n","\n","\n"]}],"source":["# Beam-search text generation:\n","sample_outputs = model.generate(generated, \n","                                do_sample=True,   \n","                                max_length=MAXLEN,                                                      \n","                                num_beams=5,\n","                                repetition_penalty=5.0,\n","                                early_stopping=True,      \n","                                num_return_sequences=1\n","                                )\n","\n","for i, sample_output in enumerate(sample_outputs):\n","    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    a = len(title) \n","    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B4UjMTwWx-ky"},"source":["### Generating text with raw GPT2\n","\n","here Generate some results without fine tuning GPT2 on medical dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-12T10:09:31.702241Z","iopub.status.busy":"2022-08-12T10:09:31.701705Z","iopub.status.idle":"2022-08-12T10:09:45.139857Z","shell.execute_reply":"2022-08-12T10:09:45.138868Z","shell.execute_reply.started":"2022-08-12T10:09:31.7022Z"},"id":"kbiaQldb1RPO","trusted":true},"outputs":[],"source":["tokenizer = get_tokenier()\n","model = get_model(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-12T10:09:45.142053Z","iopub.status.busy":"2022-08-12T10:09:45.141657Z","iopub.status.idle":"2022-08-12T10:09:47.987869Z","shell.execute_reply":"2022-08-12T10:09:47.986737Z","shell.execute_reply.started":"2022-08-12T10:09:45.142016Z"},"id":"H1ag9Z0iZbzG","outputId":"1aab1e57-e34c-4988-9575-994d790c03cc","trusted":true},"outputs":[],"source":["prompt = title\n","\n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","device = torch.device(\"cuda\")\n","generated = generated.to(device)\n","\n","model.eval()\n","sample_outputs = model.generate(generated, \n","                                do_sample=True,   \n","                                max_length=MAXLEN,                                                      \n","                                num_beams=5,\n","                                repetition_penalty=5.0,\n","                                early_stopping=True,      \n","                                num_return_sequences=1\n","                                )\n","\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
